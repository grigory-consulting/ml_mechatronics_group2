{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4097420",
   "metadata": {},
   "source": [
    "# GPT2 -> Generative Pretrained Transformer (2020 - 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880368da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97e3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass # reduced class\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import functional as F \n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd) # create Q, K, V \n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T , C = x.size() # batch_size, T .. sequence length , C = n_embd \n",
    "        qkv = self.c_attn(x)\n",
    "        q,k,v = qkv.split(self.n_embd, dim = 2) \n",
    "        # n_head = nh, C = nh * hs, hs is head size (in our case 64)\n",
    "        # C = 768, n_head = 12, hs = 64\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q,k,v, is_causal=True) # causal = masked \n",
    "        # Attention(q,k,v) = softmax(q*k/sqrt(C))*V \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # connect the result of heads again in to one vector\n",
    "        # output projection \n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
    "        self.gelu = nn.GELU() # Gaussian Error Linear Units\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config) # Causal Self-Attention = Masked Self-Attention \n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config) # MLP = Positionwise FFN\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x)) # Prenorm \n",
    "        x = x + self.mlp(self.ln_2(x)) \n",
    "        return x \n",
    "\n",
    "@dataclass \n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # maximum number of positions = context window \n",
    "    vocab_size: int = 50257 # number of tokens (gpt2-tokenizer)\n",
    "    n_embd: int = 768 # embedding dimension (tokens -> vectors)\n",
    "    n_layer: int = 12 # number of transformer layers \n",
    "    n_head: int = 12 # number of parallel heads in attention layer \n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd), # token embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd), # position embedding\n",
    "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "            ln_f = nn.LayerNorm(config.n_embd), \n",
    "        )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # last decision -> next token \n",
    "    \n",
    "    def forward(self, idx, targets = None): \n",
    "        # idx = (B,T) ... B is batch_size, T is sequence length \n",
    "        B,T = idx.size()\n",
    "        assert T <= self.config.block_size \n",
    "        pos = torch.arange(0, T, dtype = torch.long, device=idx.device)\n",
    "        tok_emb = self.transformer.wte(idx) \n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = tok_emb + pos_emb \n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x=block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # FC \n",
    "        loss = None \n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5deec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('goethe.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8e78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65ab86e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 66404 tokens\n",
      "1 epoch = 129 batches\n",
      "step 0, loss: 11.0089111328125, dt: 181.85 ms, tokens/sec: 2815.43\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "import time \n",
    "train_loader = DataLoaderLite(B=8, T=64)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_rt_seqs = 1\n",
    "max_len  = 100\n",
    "\n",
    "model = GPT(GPTConfig)\n",
    "\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "\n",
    "model.to(device)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps= 10e-8)\n",
    "for i in range(1):\n",
    "    t0=time.time()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y= x.to(device), y.to(device)\n",
    "    with torch.autocast(device_type=device, dtype = torch.bfloat16): # 16 bit floating point\n",
    "        logits,loss = model(x,y)\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    dt = time.time() - t0\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    print(f\"step {i}, loss: {loss.item()}, dt: {dt*1000:.2f} ms, tokens/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f00f9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> FAUST Here\n",
      "L will do you, \n",
      "Be your heart, we of love her.\t\t\t\t\t\t\t\n",
      "\n",
      "MARGARET Than his hand,\n",
      "That draws in the love inside,\n",
      "And let him, there’s quite so without rest,\n",
      "Only he has gone,\n",
      "Your heart such a you, you,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "Then through be so\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "#tokens = enc.encode(\"You are helpful assistant. What is 2+2?\")\n",
    "tokens = enc.encode(\"FAUST\") # Prompt\n",
    "tokens = torch.tensor(tokens, dtype = torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_rt_seqs, 1)\n",
    "x = tokens.to(device)\n",
    "\n",
    "#torch.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_len:\n",
    "    with torch.no_grad():\n",
    "        logits,loss = model(x)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "        top_probs, top_ind = torch.topk(probs, 50, dim = -1)\n",
    "        ix = torch.multinomial(top_probs,1)\n",
    "        xcol = torch.gather(top_ind, -1, ix)\n",
    "        x = torch.cat((x,xcol), dim = 1)\n",
    "#\n",
    "for i in range(num_rt_seqs):\n",
    "    tokens = x[i, :max_len].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
